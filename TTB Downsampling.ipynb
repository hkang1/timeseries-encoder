{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 276,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import numpy as np\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_csv_file (filename, path='data/'):\n",
    "    csv_reader = csv.reader(open(path + filename, 'r'))\n",
    "    rows = np.array(list(csv_reader)[1:])\n",
    "    data = [(lambda x: [ x[0].astype(int), x[1].astype(float), x[2].astype(float) ])(r) for r in rows]\n",
    "    return np.array(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 278,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# read CSV file\n",
    "data = read_csv_file('420ba246.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_point (point, base_x):\n",
    "    return [ point[0] - base_x, *point[1:] ]\n",
    "\n",
    "def linear_regression (data, base_x):\n",
    "    sums = { \"x\": 0., \"y\": 0., \"xy\": 0., \"xs\": 0., \"ys\": 0. }\n",
    "    \n",
    "    for d in data:\n",
    "        npx = normalize_point(d, base_x)\n",
    "        sums['x'] += npx[0]\n",
    "        sums['y'] += npx[1]\n",
    "        sums['xy'] += npx[0] * npx[1]\n",
    "        sums['xs'] += npx[0] ** 2\n",
    "        sums['ys'] += npx[1] ** 2\n",
    "    \n",
    "    denominator = len(data) * sums['xs'] - sums['x'] * sums['x']\n",
    "    m = (len(data) * sums['xy'] - sums['x'] * sums['y']) / denominator\n",
    "    b = (sums['y'] * sums['xs'] - sums['x'] * sums['xy']) / denominator\n",
    "    \n",
    "    return m, b\n",
    "\n",
    "def sum_of_squared_errors (data, m, b, base_x):\n",
    "    sse = 0.\n",
    "    for i, d in enumerate(data):\n",
    "        npx = normalize_point(d, base_x)\n",
    "        y = m * npx[0] + b\n",
    "        e = npx[1] - y\n",
    "        sse += e ** 2\n",
    "    return sse\n",
    "\n",
    "def triangle_area (p1, p2, p3):\n",
    "    return abs((p1[0] * (p2[1] - p3[1]) + p2[0] * (p3[1] - p1[1]) + p3[0] * (p1[1] - p2[1])) / 2.)\n",
    "    \n",
    "def split_bucket (data, threshold):\n",
    "    buckets = [None] * threshold\n",
    "    size = (len(data) - 2) / (threshold - 2)\n",
    "    for i, d in enumerate(data):\n",
    "        if i == 0:\n",
    "            buckets[i] = { \"data\": [d], \"sse\": None }\n",
    "        elif i == len(data) - 1:\n",
    "            buckets[threshold - 1] = { \"data\": [d], \"sse\": None }\n",
    "        else:\n",
    "            bi = math.floor(i / size) + 1\n",
    "            if buckets[bi] is None:\n",
    "                buckets[bi] = { \"data\": [d], \"sse\": None }\n",
    "            else:\n",
    "                buckets[bi]['data'].append(d)\n",
    "    return buckets\n",
    "\n",
    "def average_bucket_point (data, base_x):\n",
    "    count = len(data)\n",
    "    sums = [ 0, 0 ]\n",
    "    for d in data:\n",
    "        npx = normalize_point(d, base_x)\n",
    "        sums[0] += npx[0]\n",
    "        sums[1] += npx[1]\n",
    "    return sums\n",
    "\n",
    "def rank_triangle_three_buckets (buckets, base_x):\n",
    "    points = []\n",
    "    \n",
    "    for index, bucket in enumerate(buckets):\n",
    "        if index == 0 or index == len(buckets) - 1:\n",
    "            points.append(bucket['data'][0])\n",
    "        else:\n",
    "            # Set the previous point to be the last selected point\n",
    "            point_prev = normalize_point(points[-1], base_x)\n",
    "            \n",
    "            # Set the next point to be the average point of the next bucket\n",
    "            bucket_next = buckets[index + 1]\n",
    "            point_next = average_bucket_point(bucket_next['data'], base_x)\n",
    "            \n",
    "            # Find the bucket point that will maximize the triangle area\n",
    "            ttb = { \"point\": None, \"area\": 0 }\n",
    "            \n",
    "            for d in bucket['data']:\n",
    "                npx = normalize_point(d, base_x)\n",
    "                area = triangle_area(point_prev, npx, point_next)\n",
    "                if area > ttb['area'] or ttb['point'] is None:\n",
    "                    ttb = { \"point\": d, \"area\": area }\n",
    "            \n",
    "            points.append(ttb['point'])\n",
    "\n",
    "    return np.array(points)\n",
    "    \n",
    "def largest_triangle_dynamic (data, threshold=512):\n",
    "    if len(data) < threshold:\n",
    "        return data\n",
    "    \n",
    "    buckets = split_bucket(data, threshold)\n",
    "    \n",
    "    # Use the base x date object to use as an offset to normalize regression line calculation\n",
    "    base_x = data[0][0]\n",
    "    \n",
    "    # Number of iterations of merging or spliting buckets based on SSE\n",
    "    iterations = math.floor(len(data) * 10 / threshold)\n",
    "    \n",
    "    for i in range(iterations):\n",
    "        min_sse = { \"index\": None, \"value\": None }\n",
    "        max_sse = { \"index\": None, \"value\": None }\n",
    "        \n",
    "        for index, bucket in enumerate(buckets):\n",
    "            bucket_prev = buckets[index - 1] if index != 0 else None\n",
    "            bucket_next = buckets[index + 1] if index != len(buckets) - 1 else None\n",
    "            \n",
    "            # Skip buckets previously calculated and not previously merged/split\n",
    "            if bucket['sse'] is None:\n",
    "                # Form overlapping buckets (includes one data point from previous and next buckets)\n",
    "                overlapping_bucket = bucket['data'].copy()\n",
    "                \n",
    "                # Add the last point from the previous bucket if applicable\n",
    "                if bucket_prev:\n",
    "                    overlapping_bucket.insert(0, bucket_prev['data'][-1])\n",
    "                \n",
    "                # Add the first point from the next bucket if applicable\n",
    "                if bucket_next:\n",
    "                    overlapping_bucket.append(bucket_next['data'][0])\n",
    "                \n",
    "                # Calculate linear regression for overlapping buckets\n",
    "                m, b = linear_regression(overlapping_bucket, base_x)\n",
    "                \n",
    "                # Calculate the sum of squared errors to determine which buckets to split or merge\n",
    "                bucket['sse'] = sum_of_squared_errors(overlapping_bucket, m, b, base_x)\n",
    "            \n",
    "            # Find the bucket with the largest SSE with more than one item (to allow a split)\n",
    "            if max_sse['value'] is None or (bucket['sse'] > max_sse['value'] and len(bucket['data']) > 1):\n",
    "                max_sse['index'] = index\n",
    "                max_sse['value'] = bucket['sse']\n",
    "            \n",
    "            # Find a pair of adjacent buckets with the smallest SSE\n",
    "            if index != 0:\n",
    "                sum_sse = bucket_prev['sse'] + bucket['sse']\n",
    "                if min_sse['value'] is None or sum_sse < min_sse['value']:\n",
    "                    min_sse['index'] = index - 1\n",
    "                    min_sse['value'] = sum_sse\n",
    "            \n",
    "        # Because we are spliting the largest SSE bucket first\n",
    "        # if the merging buckets come after split buckets,\n",
    "        # the index should go up by one\n",
    "        if min_sse['index'] and min_sse['index'] > max_sse['index']:\n",
    "            min_sse['index'] += 1\n",
    "\n",
    "        # Split the largest bucket\n",
    "        bucket_max = buckets[max_sse['index']]\n",
    "        mid_index = round(len(bucket_max['data']) / 2.)\n",
    "        half0 = { \"data\": bucket_max['data'][0:mid_index], \"sse\": None }\n",
    "        half1 = { \"data\": bucket_max['data'][mid_index:], \"sse\": None }\n",
    "        buckets = [ *buckets[:max_sse['index']], half0, half1, *buckets[max_sse['index']+1:] ]\n",
    "\n",
    "        # Merge the smallet buckets\n",
    "        bucket_min0 = buckets[min_sse['index']]\n",
    "        bucket_min1 = buckets[min_sse['index']+1]\n",
    "        merged = { \"data\": [ *bucket_min0['data'], *bucket_min1['data'] ], \"sse\": None }\n",
    "        buckets = [ *buckets[:min_sse['index']], merged, *buckets[min_sse['index']+2:] ]\n",
    "        \n",
    "    return rank_triangle_three_buckets(buckets, base_x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 280,
   "metadata": {},
   "outputs": [],
   "source": [
    "points = largest_triangle_dynamic(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
